[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "open_perception"
version = "0.1.0"
description = "An open-vocabulary perception pipeline with multiple VLMs."
authors = [
    { name="project grappa", email="grappa993@gmail.com" }
]
license = { text = """
MIT License

Copyright (c) 2024
"""}
readme = "README.md"
requires-python = ">=3.8"
keywords = ["computer-vision", "object-detection", "segmentation"]

dependencies = [
    "transformers<4.51.0",
    "open3d",
    "scipy",
    "redis",
    "jupyter>=1.0.0",
    "tqdm",
    "pyyaml",
    "opencv-python>=4.5",
    "torch",
    "torchvision",
    "torchaudio",
    "moviepy",
    "easydict",
    "datasets",
    "litellm",
    "fpdf2",
    "tkinterdnd2",
    #"sam-2 @ file:./third_party/segment-anything-2-real-time"
]

[tool.uv.sources]
torch = { index = "pytorch" }
torchvision = { index = "pytorch" }
torchaudio = { index = "pytorch" }
sam-2 = { path = "./third_party/segment-anything-2-real-time", editable = true }

[[tool.uv.index]]
name = "pytorch"
url = "https://download.pytorch.org/whl/cu124"
explicit = true
default = false

[project.optional-dependencies]
realsense = ["pyrealsense2"]
yoloworld = ["ultralytics"]
lookup = ["sentence_transformers"]
dev = ["pytest", "pytest-subtests"]
redis_client = ["opencv-python>=4.5", "moviepy", "redis"]
all = ["pyrealsense2", "ultralytics", "pytest", "pytest-subtests", "opencv-python>=4.5", "moviepy", "redis", "sentence_transformers"]

